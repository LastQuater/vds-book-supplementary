---
title: "Cleaning the NHANES diabetes data"
subtitle: "[DSLC stages]: Data cleaning and pre-processing"
format: 
  html:
    toc: true
    toc-location: right
    number-depth: 4
    theme: cerulean
    df-print: kable
execute:
  echo: true
editor: source
number-sections: true
embed-resources: true
---


Start by loading in any libraries that you will use in this document.

```{r setup}
#| label: setup
#| message: false
#| warning: false

# load the libraries we will need in this document
library(tidyverse)
library(lubridate)
library(janitor)
library(fastDummies)
```


## Domain problem formulation


Write a summary of the problem.


## Data source overview

Briefly describe where the data being used for this project came from

## Step 1: Review background information {#sec:bg-info}


### Information on data collection 

Write a summary of how the data was collected.

### Data dictionary

If there is a data dictionary, give some details here.

### Answering questions about the background information

Answer the recommended background information questions from the Data Cleaning chapter.

- *What does each variable measure?* 

- *How was the data collected?* 

- *What are the observational units?* 

- *Is the data relevant to my project?*



## Step 2: Loading the data

Load the data

```{r}
diabetes_orig <- read_csv("../data/samadult.csv")
```

Let's look at the first few rows to make sure it looks like it has been loaded in correctly:

```{r}
head(diabetes_orig)
```

And let's examine the dimension of the data.

```{r}
dim(diabetes_orig)
```

That's a lot of data!





## Step 3: Examine the data

In this section we explore the common messy data traits to identify any cleaning action items.



### Finding invalid values

#### Numeric variables


#### Categorical variables


### Examining missing values




### Examining the data format


### Assessing column names


### Assessing variable type



### Evaluating data completeness


### Answering any unanswered questions





## Step 4: Prepare the data

Don't forget to split the data into training, validation and test sets before you clean and pre-process it!